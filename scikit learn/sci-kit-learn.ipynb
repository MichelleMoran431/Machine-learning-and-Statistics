{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d30f05e-4023-4029-b06c-8654ebb18a55",
   "metadata": {},
   "source": [
    "## Overview of the Scikit-Learn Python Library ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eae333-5739-49e0-81e2-6939c1f424cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "<kbd>\n",
    "<center><img src=\"machine learning examples.png\" alt=\"drawing\" width=\"800\"/><Center>\n",
    "\n",
    "<center> ref : https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/machine-learning.png <center>\n",
    "    </kbd>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e2478-36d1-4a12-a147-87e73e2fbf1c",
   "metadata": {},
   "source": [
    "### What is Sciki-learn ? <sup>[1][2][3]</sup>\n",
    "\n",
    "A library for machine learning in python.Scikit learn started as a project in 2007 and in 2010 it made its first public release. A international community of experts has been leading its development. According to its website , it used by various international companies such as Spotify to Booking.com right down to me , a data analyst student for her model project.\n",
    "\n",
    "Sciki-learn is built on NumPy , SciPy and matplotlib , these are the tools for data analysis and data mining where :\n",
    "\n",
    " - Using NumPy for array vectorization , \n",
    " - Pandas for accessing dataframes \n",
    " - matplotlib for plotting, \n",
    " - Scipy for scientific computing\n",
    "\n",
    "\n",
    "Providing the following :\n",
    "\n",
    "Simple and efficient tools for predictive data analysis\n",
    "\n",
    "To start at the very beginning , a learning problem considers a set of n samples of data and then tries to predict properties of unknown data from that set. The n samples can have several attributes or features i.e multivariate data.\n",
    "\n",
    "Machine learning evaluates an algorithm by splitting the data set into two :\n",
    "\n",
    "- training set - properties are learned\n",
    "- testing set - learned properties are tested \n",
    "\n",
    "\n",
    "Sciki-learn library focuses on modelling this data from learning problems using the following few catergories:\n",
    "\n",
    "#### ** 1.Supervised learning - used by majority of machine learning\n",
    "\n",
    "This is where you have labelled input (x) and output (y) variables and you use an algorithm to learn the mapping function from the input to the output.\n",
    "\n",
    "With the main aim to approximate this mapping function when you have new input data to predict the output variables for that new data . It is called supervised as the process of algorithm learning from the training dataset can be thought of as a teacher supervising the learning process. The algorithm can be corrected if needs be during the predicive process.\n",
    "\n",
    "Supervised learning can be further grouped into :\n",
    "\n",
    "- Classification - the output variable (y) is a catergory \n",
    "- Regression - the output variable (y) is a real value \n",
    "\n",
    "##### **Popular application:**\n",
    "\n",
    " - Predictive analytics (house prices, stock exchange prices, etc.) \n",
    " - Text recognition \n",
    " - Spam detection \n",
    " - Customer sentiment analysis \n",
    " - Object detection (e.g. face detection)\n",
    "\n",
    "#### **2. Unsupervised learning\n",
    "\n",
    "Here you only have unlabelled input data (x) and no corresponding output variables\n",
    "\n",
    "The aim of this type of learning is to model the underlying distribution in the data in order to learn more about the data. Its unsupervised meaning , there is no correct answers and no monitoring by a teacher unlike the supervised learning above. Algorithms discover and learn to inherent structure from the input data with no guidance.\n",
    "\n",
    "Unsupervised learning can be further grouped into :\n",
    "\n",
    "- Clustering- a problem where you want to discover the inherent groupings in the data i,e purchasing power\n",
    "- Association - a problem where you want to discover the rules that describe large portions of your data such as people that buy X also tend to buy Y\n",
    "\n",
    "##### **Popular application:**\n",
    "\n",
    "Anomaly Detection customer behavour prediction noise removal from the dataset\n",
    "\n",
    "#### **Other modelling examples:**\n",
    "\n",
    "Cross Validation − It is used to check the accuracy of supervised models on unseen data.\n",
    "\n",
    "Dimensionality Reduction − It is used for reducing the number of attributes in data which can be further used for summarisation, visualisation and feature selection.\n",
    "\n",
    "Ensemble methods − As name suggest, it is used for combining the predictions of multiple supervised models.\n",
    "\n",
    "Feature extraction − It is used to extract the features from data to define the attributes in image and text data.\n",
    "\n",
    "Feature selection − It is used to identify useful attributes to create supervised models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc915a-7f91-4825-9142-24ebd4338592",
   "metadata": {},
   "source": [
    "## Dataset : The World Happiness Report <sup>[4][5][6]</sup>\n",
    "\n",
    "The 2021 World Happiness Report ( 9th ) is  based on a wide variety of data , with the most important source been the Gallup World Poll. \n",
    "The life evaluations from this poll provide the basis for the annual happiness rankings. It acknowledges that COVID-19 has posed unique problems in data collection. \n",
    "        \n",
    "        \n",
    "The purpose of the World Happiness Report 2021 was to focus on the effects of COVID-19 and how people all over the world have fared. \n",
    "\n",
    "  - The pandemic's worst effect has been the 2 million deaths from COVID-19 in 2020. A rise of nearly 4% in the annual number of deaths worldwide represents a serious social welfare loss.\n",
    "  - For the living there has been greater economic insecurity, anxiety, disruption of every aspect of life, and, for many people, stress and challenges to mental and physical health."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f72da1-834c-4b73-af50-03dacbe16d71",
   "metadata": {},
   "source": [
    "### Variables : <sup>[6]</sup>\n",
    "\n",
    "NB: We will only be concentrating on the following 7 for the purposes of this analysis:\n",
    "\n",
    "**Ladder score :** or called the **Happiness score**. It is the national average response to the question of life evaluations. So the imagine of a ladder with 0 to 10 steps, 0 representing the worst possible life and 10 the best possible life\n",
    "\n",
    "**GDP per capita :** Gross domestic product - monetary value of all finished goods and servies made within a country during a specific time period\n",
    "\n",
    "**Healthy Life Expectancy:** based on data from the WHO Global Health Observatory data repository\n",
    "\n",
    "**Social Support :** a person has family or friends they can count on in times of trouble. Measured as Binary responses ( either 0 or 1). yes or no\n",
    "\n",
    "**Freedom to make life choices :** this is the national average of responses to the GWP question , Are you satisfied or not with your freedom to choose what you do with your life\n",
    "\n",
    "**Generosity:** resodial of regressing the national average of response to the GWP question : have you donated money to charity in the last month ?\n",
    "\n",
    "**Corruption Perception: 8** measure of the national average of the survery responses to 2 questions - is corruption widespread through your government and 2nd businesses. Overall perception measured as an average of 0 or 1 responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68120e47-1493-4637-8c09-3a9d6451006b",
   "metadata": {},
   "source": [
    "## SETUP ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8b5b6-ba42-4c19-9ad1-2d810d70070e",
   "metadata": {},
   "source": [
    "## Step 1 :Importing Packages ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38deadb-f4c5-4e33-b6d2-bcd1999147fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical arrays.\n",
    "import numpy as np\n",
    "\n",
    "# Data frames.\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logistic regression.\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "# K nearest neighbours.\n",
    "import sklearn.neighbors as nei\n",
    "\n",
    "# Helper functions.\n",
    "import sklearn.model_selection as mod\n",
    "\n",
    "# Fancier, statistical plots.\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from chart_studio.plotly import plot, iplot\n",
    "from plotly.offline import iplot\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import max_error\n",
    "\n",
    "\n",
    "# import statistic library\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6aff59-5f78-49a5-a33b-30ea49a37c73",
   "metadata": {},
   "source": [
    "## Step 2. Read in the Dataset <sup>[7]</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b7f11-d5f3-497c-9f23-e72f3119b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the datasets Using pandas  - then check if the data works ok with the scikit learn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "happiness = pd.read_csv(r'C:\\Users\\User\\Desktop\\repo\\Machine-learning-and-Statistics\\scikit learn\\Happiness 2021 2.csv')\n",
    "happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67b3b2-ef3d-4017-bfad-85558ec6666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c72df2-fc6c-4321-ad50-86a31bec66e0",
   "metadata": {},
   "source": [
    "The Ladder score ( happiness scores) mean is 5.53 ,which indicates mid range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbba38f-6546-4f55-8beb-7e859b4db40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing information on the dataset \n",
    "happiness.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757970f-3cd5-4dfe-a5b2-d3b529a89711",
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.shape\n",
    "# 20 columns with 149 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f59ad-cda8-422a-adf0-f8ed779574f5",
   "metadata": {},
   "source": [
    "#### Show an world overview of the ladder score using plotly and chloropleth <sup>[8][9]</sup>\n",
    "\n",
    "Plotly can be used to create an interactive map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1766cf5-9240-46a4-a472-1213e88e94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.choropleth(data_frame=happiness,\n",
    "                    locations=\"Country name\",\n",
    "                    locationmode=\"country names\",\n",
    "                    color=\"Ladder score\",\n",
    "                    title=\"Happiness score per Country\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25331f-825a-424e-8595-c8811d4f80b9",
   "metadata": {},
   "source": [
    "## Step 3 :PERFORMING EXPLORATORY ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff5e6a-d4b7-4f42-ae70-bbfbb15cbbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selecting which features to Keep or Drop ###\n",
    "# There is 20 columns , we do not need all information from them \n",
    "# using the drop function to select variables to visualise in the heatmap\n",
    "happiness = happiness.drop(columns=happiness.columns[12:])\n",
    "happiness = happiness.drop(columns=[ 'upperwhisker', 'lowerwhisker', 'Standard error of ladder score'])\n",
    "\n",
    "#### ** 1. Correlations <sup>[10]</sup>\n",
    "corr = happiness.corr()\n",
    "f,ax =plt.subplots (figsize=(15,10))\n",
    "sns.heatmap(corr, annot=True,fmt='.2f', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029d78b-f603-4772-bd2f-e939c38cafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the correlation against the ladder score\n",
    "correlation_matrix = happiness.corr()\n",
    "correlation_matrix[\"Ladder score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15071c-cf2e-492b-8309-0da281de5036",
   "metadata": {},
   "source": [
    "#### **Heatmap Correlation conclusions:**\n",
    "\n",
    "Using the correlation information from the above heatmap , we can decide what the target variables can be , which I think is the region rather than the country ( country only appears once). The other variables are ladder score , Socal supports, healthy life expectancy, logged GDP per capta and freedom to make life choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac5df6-1bb9-47cb-a8cf-80d870f40ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the data again after dropping columns\n",
    "happiness.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5057cf6-124f-49f2-be2c-990698504dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a filter to locate information on irelands happiness\n",
    "happiness[happiness['Country name']== 'Ireland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0b794-978b-4ac1-aac6-5c208a05ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    ">>> happiness[\"Ladder score\"].hist(bins=20)\n",
    ">>> plt.show()\n",
    "\n",
    "## the histogram shows the highest degree of happiness on between 3 and 8 with a dip at 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5964a35-f7f0-4dfd-b4c9-e7460d8d4953",
   "metadata": {},
   "source": [
    "## Step 3 :PERFORM DATA VISUALIZATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a4cdf-a568-4121-a8bf-a07076b54d5d",
   "metadata": {},
   "source": [
    "1. Pairplot visualization <sup>[11]</sup>\n",
    "\n",
    "It visualizes data to find the relationship between them where the variables can be continous or categorical. After using a correlation matrix above ,we determined what variables are related closely to the Ladder score ( happiness). Using the seaborn package library , we created a pairplot. \n",
    "\n",
    "Looking at the pairplot below , there seems to be linear relation between ladder score and logged GDP per captia , Healthy life expectancy and social support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490a69d-a68d-4b5e-8a23-434bf8e8feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## plotting the pair plot \n",
    "\n",
    "fig=plt.figure(figsize = (20,20))\n",
    "sns.pairplot(happiness[['Ladder score','Logged GDP per capita','Social support','Healthy life expectancy','Freedom to make life choices','Generosity','Perceptions of corruption']])\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d18888-acfa-46b8-8a98-a5352875363b",
   "metadata": {},
   "source": [
    "2. Distplot visualization <sup>[12]</sup>\n",
    "\n",
    "We used a distplot to visualize the data distribution of variables \n",
    "\n",
    "Conclusions : \n",
    "\n",
    "Ladder score - there seems to a degree of bimodal in its distribution , it shows the highest ladder score to be around 6 but also around 4.5 \n",
    "Logged GDP per capita - bimodal in distribution - majority of data is in the range of 9 and 12\n",
    "Social support - there is a clear distribution where the high range of social support between .8 and 1.0\n",
    "Healthy life expectancy - the highest result is 70's clearly\n",
    "Freedom to make life choices - bimodal distribution\n",
    "Generosity - bimodal distribution\n",
    "Perceptions of corruption'- a normal distribution \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4462fce-5653-4cda-998f-910cb0613231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printout the displot - distribution plot \n",
    "#displot combines the matplotlib.hist function with seaborn kdeplot() for each of the interested columns using a for loop to create these subplots\n",
    "\n",
    "columns = ['Ladder score','Logged GDP per capita','Social support','Healthy life expectancy','Freedom to make life choices','Generosity','Perceptions of corruption']\n",
    "plt.figure(figsize = (20, 50))\n",
    "for i in range(len(columns)):\n",
    "  plt.subplot(8, 2, i+1)\n",
    "  sns.distplot(happiness[columns[i]], color = 'r');\n",
    "  plt.title(columns[i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ed892-a4f5-4ea1-8f22-a1e53e1e43d0",
   "metadata": {},
   "source": [
    "3. Scatter visualization <sup>[13]</sup\n",
    "\n",
    "Using the plotly package to give an interaction on the plots ; so you can relate the country with its score and life expectancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10c329-281f-4319-867d-4761f21403f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between score, Healthy life expectancy and country\n",
    "fig = px.scatter(happiness, x = 'Healthy life expectancy', y = 'Ladder score', color = 'Country name', trendline = \"ols\", hover_name = \"Country name\")\n",
    "fig.update_layout ( title_text = 'Ladder score vs Healthy life expectancy')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308757d6-602f-403f-9ab6-d03502a0567c",
   "metadata": {},
   "source": [
    "## SciKit-Learn Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0bf582-d656-4bc1-8c85-05fec56864fa",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b240f4e-c415-4df4-b71c-5493e4f154f6",
   "metadata": {},
   "source": [
    "## SciKit-Learn Algorithm - K- Means Clustering <sup>[14][15]</sup>\n",
    "\n",
    "\n",
    "An example of an unsupervised Algorithm. It works by grouping some data points with similar attribute values together ( clustering) by measuring the Euclidian distance ( distance between two points)between the points unsupervised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674f99a-196d-4bfc-b0b5-cb60c06131a0",
   "metadata": {},
   "source": [
    "\n",
    "Aims : \n",
    "\n",
    "- To train an unsupervised machine learning algorithm known as k means clustering to cluster countries based on features such as economic production , social suppport , life expectancy , freedom , absense of corruption and generosity\n",
    "\n",
    "How it works : \n",
    "\n",
    "1. Choose the number of clusters \"k\"\n",
    "2. Select random K points that are going to be the centroids for each cluster\n",
    "3. Assign each data point to the nearest centroid - enable to create \"k\" number of clusters\n",
    "4. Calculate a new centroid for each cluster\n",
    "5. Reassign each data point to the new closet centroid\n",
    "6. Go to step 4 and repeat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e5a7b-760e-4585-b908-386aa46366a8",
   "metadata": {},
   "source": [
    "#### **Step 1 : Data preparation  for feeding into the clustering model <sup>[16]</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1649a2c-e87a-43da-91d8-81029e3b0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create clusters without the use of Ladder score to see which countries fall under similar clusters\n",
    "# Removing the following columns from the dataset - Country name, regional indicator and Ladder score  so that all we have is table of inputs with no relation to a target class or score\n",
    "\n",
    "df_happiness= happiness.drop(columns = ['Country name', 'Regional indicator', 'Ladder score'])\n",
    "df_happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d2e51a-79e9-4e0a-889b-be5c00a7fff1",
   "metadata": {},
   "source": [
    "#### **Step 2 : Scale the data <sup>[17]</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4927ada-f329-4ec8-a107-138ca9b81337",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_happiness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f25e7-35ee-4be3-a71a-1895c1cf4f3b",
   "metadata": {},
   "source": [
    "#### **Step 3  : Finding the correct no. of clusters <sup>[18]</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc2609-172a-4cad-84bd-7aa2120eab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the correct no. of clusters to use using the elbow method - so called because the line chart looks like a elbow on the arm\n",
    "#Using the equation called Within Cluster Sum of Squares ( WCSS) - measuring the distance between the input points and the centroid at difference cluster values\n",
    "\n",
    "\n",
    "WCSS = []\n",
    "range_values = range(1,20) # look at 20 K values to start \n",
    "\n",
    "# use a for loop to apply the different ranges of k value to the k- means clustering algorithn\n",
    "\n",
    "for i in range_values:\n",
    "    kmeans = KMeans(n_clusters = i) # KMeans imported from sklearn.cluster\n",
    "    kmeans.fit(scaled_data)\n",
    "    WCSS.append(kmeans.inertia_)\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(WCSS, 'bx-')\n",
    "plt.title('Finding right number of clusters')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('WCSS') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccce5a4-1b70-4abe-b79d-039601b3c534",
   "metadata": {},
   "source": [
    "#### **Step 4 : Applying the K- Means Method <sup>[19]</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64511be3-4ff5-4079-9c95-099e9156586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The no. of clusters determined is 3\n",
    "\n",
    "kmeans = KMeans(3)\n",
    "kmeans.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05833eea-0cf7-4031-8578-30b54f368223",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532db74-e9f5-4652-8baf-06ca14077914",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd41db-8a50-4096-8fec-ef786ba796b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the cluster center information to our dataset.This data is scaled down \n",
    "\n",
    "cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [df_happiness.columns])\n",
    "cluster_centers      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631ce84-8c2f-471c-aa14-d3fd5188de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to understand what these numbers mean, let's perform inverse transformation - scale these numbers back \n",
    "cluster_centers = scaler.inverse_transform(cluster_centers)\n",
    "cluster_centers = pd.DataFrame(data = cluster_centers, columns = [df_happiness.columns])\n",
    "cluster_centers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d339c-0364-455d-a722-13a5e0ac8b5d",
   "metadata": {},
   "source": [
    "Cluster 0: countries that have GDP in the range of 0.6 to 1.4 and have high social support. These countries have medium life expectancy and have high freedom to make life choices. These counties have low generosity and high perception of corruption.\n",
    "\n",
    "Cluster 1: countries that have very high GDP, high social support and high life expectancy. These counties have high freedom to make life choices, medium generosity and low perception of corruption.\n",
    "\n",
    "Cluster 2: countries that have low GDP average life expectancy and average social support. These counties have low freedom to make life choices, high generosity and medium perception of corruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996b6d0-5f3a-47d7-a5ee-1068a53924aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape # Labels associated to each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fd097-1050-4d3f-92e6-291b1197b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying our inputs from the scaled_data to the kmeans trained model to determine which cluster a country belongs to\n",
    "\n",
    "y_kmeans = kmeans.fit_predict(scaled_data)\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51441874-b50f-4701-b529-976171bef0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the clusters labels to our original dataframe\n",
    "happy_df_cluster = pd.concat([happiness, pd.DataFrame({'cluster':labels})], axis = 1)\n",
    "happy_df_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8ace2-8692-47e0-bda8-b4a07530b0a2",
   "metadata": {},
   "source": [
    "#### ** Visualisation of the clusters in our happiness dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360d1fa-4a92-48e9-858c-c5e812b5031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the histogram of various clusters - showing the different distributions \n",
    "for i in df_happiness.columns:\n",
    "  plt.figure(figsize = (35, 10))\n",
    "  for j in range(3):\n",
    "    plt.subplot(1,3,j+1)\n",
    "    cluster = happy_df_cluster[happy_df_cluster['cluster'] == j]\n",
    "    cluster[i].hist(bins = 20)\n",
    "    plt.title('{}    \\nCluster {} '.format(i, j))\n",
    "  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5012ba1-5fce-4559-82cd-76bd657b0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between cluster and score \n",
    "\n",
    "fig = px.scatter(happy_df_cluster, x = 'cluster', y = \"Ladder score\",\n",
    "            color = \"Country name\", hover_name = \"Regional indicator\")\n",
    "          \n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Happiness Score vs Cluster'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da44bd-8ce0-4f62-9c3b-c9c954b57c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between cluster and GDP\n",
    "\n",
    "fig = px.scatter(happy_df_cluster, x='cluster', y='Logged GDP per capita',\n",
    "            color = \"Country name\", hover_name = \"Regional indicator\")\n",
    "       \n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='GDP vs Clusters'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf44415-7ff2-405c-a547-b1498fa40882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visaulizing the clusters with respect to economy, corruption, gdp, rank and their scores\n",
    "\n",
    "from bubbly.bubbly import bubbleplot\n",
    "\n",
    "figure = bubbleplot(dataset=happy_df_cluster, \n",
    "    x_column='Logged GDP per capita', y_column='Perceptions of corruption', bubble_column='Regional indicator',  \n",
    "    color_column='cluster', z_column='Healthy life expectancy', size_column='Ladder score',\n",
    "    x_title=\"Logged GDP per capita\", y_title=\"Corruption\", z_title=\"Life Expectacy\",\n",
    "    title='Clusters based Impact of Economy, Corruption and Life expectancy on Happiness Scores of Nations',\n",
    "    colorbar_title='Cluster', marker_opacity=1, colorscale='Portland',\n",
    "    scale_bubble=0.8, height=650)\n",
    "\n",
    "iplot(figure, config={'scrollzoom': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6945c9-1e72-4ed0-8d69-d8135afc43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the clusters geographically\n",
    "data = dict(type = 'choropleth', \n",
    "           locations = happy_df_cluster[\"Country name\"],\n",
    "           locationmode = 'country names',\n",
    "           colorscale='RdYlGn',\n",
    "           z = happy_df_cluster['cluster'], \n",
    "           text = happy_df_cluster[\"Regional indicator\"],\n",
    "           colorbar = {'title':'Clusters'})\n",
    "\n",
    "layout = dict(title = 'Geographical Visualization of Clusters', \n",
    "              geo = dict(showframe = True, projection = {'type': 'azimuthal equal area'}))\n",
    "\n",
    "choromap3 = go.Figure(data = [data], layout=layout)\n",
    "iplot(choromap3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5adef07-e3fc-4c60-a635-d4e07473c446",
   "metadata": {},
   "source": [
    "## Data Preparation and preproccessing :¶ For KNN and Linear Regression Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3abf17-132a-4846-93e7-e886395b1621",
   "metadata": {},
   "source": [
    "## Test and Train Split \n",
    "Split the data into training and test sets in Python using scikit-learn’s built-in train_test_split():\n",
    "\n",
    "The test_size refers to the number of observations that you want to put in the training data and the test data. If you specify a test_size of 0.2, your test_size will be 20 percent of the original data, therefore leaving the other 80 percent as training data.\n",
    "\n",
    "The random_state is a parameter that allows you to obtain the same results every time the code is run. train_test_split() makes a random split in the data, which is problematic for reproducing the results. Therefore, it’s common to use random_state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78643d6c-f240-41b0-97be-4d94bb4a6d86",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e450b-e2a6-4270-baa8-f06d2fabe5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 2 objects that now contain data :x and y . \n",
    "\n",
    "x = happiness.drop(columns =['Ladder score',\"Country name\",\"Regional indicator\"])\n",
    "y = happiness['Ladder score']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652d112-0d9c-451c-8513-711880a762c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2e867-a8fe-4b27-b22e-1400193be447",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a58c72-95bf-446c-8e61-24170f98883b",
   "metadata": {},
   "outputs": [],
   "source": [
    " y_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df549cd-cb39-436b-9b5a-8bd7d5738798",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.index.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b29f21-49ef-4d55-aa95-bc724282fd0e",
   "metadata": {},
   "source": [
    "### Scaling of the data  : Standardization <sup>[20]</sup>\n",
    "\n",
    "This is a crucial part of data preprocessing stage . Here we use standardization instead of normalization as we know that the data follows a gaussian distribution. Linear regression (gradient based Algorithm) require data to be scaled and KNN is a distance based algorithm is most affected by the range of features therefore needs the data to be scaled prior to fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7972b2-9932-4946-9f54-0c6d14548598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data so that all features can contribute equally to the result. \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc=MinMaxScaler(feature_range=(0, 1))\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e602a54-8608-43f6-b237-8e4415c8ffdb",
   "metadata": {},
   "source": [
    "## SciKit-Learn Algorithm - KNN <sup>[21][22]</sup>\n",
    "\n",
    "An example of an supervised Algorithm : used for Classification and Regression\n",
    "\n",
    " - Classification - learn how to classify any new observation - target variable is catergorical\n",
    " - Regression - prediction made on numerous independent variables - target variable is numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb5294-059b-44b8-a9d8-f7b0016ebd0b",
   "metadata": {},
   "source": [
    "#### **Aim:\n",
    "\n",
    "The goal is to develop a model that can predict the happiness score base purely on the other independent variables of the dataset by applying kNN to find the closet prediction score as possible\n",
    "\n",
    "To use data that already has the answers to train its algorithm. It takes a new data point and looks at the existing data points that neighbour it. The new data point is than catergoried according to the majority of the existing neighbouring data points. the key is to determine the number of neighbours to look at , k . \n",
    "we have two types of variables at the same time , a target variable (y) and independent variables (x)\n",
    "The target variable is what we want to predict and it depends on the independent variables. \n",
    "KNN is a non linear learning algorithn - it uses any approach other than a line to separate their cases.\n",
    "\n",
    "An advantage to KNN lies in the ease of interpretation and understanding whats happening in a model and also it can be very quick to develop. Its good for cases that dont require highly complex techniques.However there are techniques available to improve KNN algorithm to aid it in projects are that highly complex like bagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c097ed-94c1-4d27-ba9d-6db0012a3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see what the happiness score ranges we expect , we visualise though a histogram, the range goes from below 3 up to 8\n",
    "import matplotlib.pyplot as plt\n",
    "happiness[\"Ladder score\"].hist(bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1167d0f7-8e27-49e9-8e37-7483449fb008",
   "metadata": {},
   "source": [
    "### Splitting Data Into Training and Test Sets for Model Evaluation : Under data processing section of this notebook\n",
    "\n",
    "- Training data - used to fit the model. For KNN . the training data will be used as neighbours\n",
    " - Test data - used to evaluate the model. For KNN , make predictions on the ladder score for a country  in the test data and compare to real data\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd56cb-1e67-48c9-8b66-fcde5572ce84",
   "metadata": {},
   "source": [
    "### Evaluation of the Model Fit\n",
    "\n",
    "The model needs to be evaluated : using RMSE ( root mean squared error) is a common way. Also looking at a range of K values between 1 and 20. As if you use one neighbour a prediction can strongly  change from one point to another. perhaps an outlier. However if you look at multiple data points , this impact is lessened. \n",
    "\n",
    "\n",
    "It is computed as follows:\n",
    "\n",
    "1. Compute the difference between each data point’s actual value and predicted value.\n",
    "2. For each difference, take the square of this difference.\n",
    "3. Sum all the squared differences.\n",
    "4. Take the square root of the summed value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85b866-c288-4ca2-97c2-b1ccd9573ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of the prediction error on the training data\n",
    "\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(20):\n",
    "    K = K+1\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=K) # unfitted model created \n",
    "    knn_model.fit(X_train, y_train)\n",
    "    train_preds = knn_model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, train_preds)\n",
    "    rmse = sqrt(mse)\n",
    "    rmse_val.append(rmse) #store rmse values\n",
    "    print('RMSE value for k= ' , K , 'is:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502210e2-f2f3-4285-9845-fc619eeffc2f",
   "metadata": {},
   "source": [
    "For a very low value of k ( suppose k=1) the model can overfit on the training data , leading to higher error rate on the validation data\n",
    "but for high value of k , the model can also perform badly, so looking above a the error rates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0fbf73-e91b-4417-99fb-7ff882dd89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the rmse values against k values\n",
    "curve = pd.DataFrame(rmse_val) #elbow curve \n",
    "curve.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d09477-1b30-4d88-a46d-f76a8b7e442c",
   "metadata": {},
   "source": [
    "Evaluate the performances on the predictive performances on the test set with the same function as before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab89bd-6e87-4af0-a352-0408156385ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_val2 = [] #to store rmse values for different k\n",
    "for K in range(20):\n",
    "    K = K+1\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=K)\n",
    "\n",
    "    knn_model.fit(X_train, y_train)\n",
    "\n",
    "    test_preds = knn_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, test_preds)\n",
    "    rmse = sqrt(mse)\n",
    "    rmse_val2.append(rmse) #store rmse values\n",
    "    print('RMSE value for k= ' , K , 'is:', rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d498b8-d530-4c34-9101-3687130e6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the rmse values against k values\n",
    "curve = pd.DataFrame(rmse_val2) #elbow curve \n",
    "curve.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95240bb5-eb66-42b7-84b4-0cbf20be140d",
   "metadata": {},
   "source": [
    "I have evaluated the error on data that wasn’t yet known by the model. This more-realistic RMSE is slightly higher than before. The RMSE measures the average error of the predicted ladder score, so you can interpret this as having, on average, an error on training data of 0.4875. Whether an improvement from 0.4875 years to 0.56266 from the test score is good is case specific.\n",
    "\n",
    "There is a relatively large difference between the RMSE on the training data and the RMSE on the test data. This means that the model suffers from overfitting on the training data: It does not generalize well.\n",
    "\n",
    "We can see if we can overcome this by optimizing the prediction error or test error using various tuning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57f8ed-a2b8-4b84-8b78-5e8490ea9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualisation of the Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840885bf-7e45-47a8-bce9-cc21873b3d00",
   "metadata": {},
   "source": [
    "## Plotting the Fit of Your Model\n",
    "\n",
    "To understand what the model has learned , you need to visualise how the predictions have been made using matplotlib: using seaborn package to create a scattor plot of columns in the X_test :looking at life expectancy and social support, which were closely correlated to the ladder score ( happiness score) . We use c = to specifify the size of the points in the scatter plot. \n",
    "Each point on the plot is a happiness score and the colour indicates the level of predicted happiness ( ladder)., so the age is on the x axis and the social is on the y axis . So the higher the life expectance and social supports the deeper colour the point is , therefore the ladder score is higher. The model is correct in its learnings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821edbe-a83a-4d9c-a4e5-fa3fb03e9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter Graph 1 : looking at the predicted values ( Test_preds) , used as the color bar. \n",
    "#Looking at Social support and Healthy life expectancy variables and ladder score\n",
    "\n",
    "import seaborn as sns\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "f, ax = plt.subplots()\n",
    "points = ax.scatter(X_test[:,1], X_test[:,2], c=test_preds, s=100, cmap=cmap)\n",
    "f.colorbar(points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6696707-6927-46ed-bbb3-34c5e8a25172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by changing the c value to y_test , we can check where the above trend observed exists in the actual values of the dataset. \n",
    "\n",
    "import seaborn as sns\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "f, ax = plt.subplots()\n",
    "points = ax.scatter(X_test[:,1], X_test[:, 2], c= y_test, s=100, cmap=cmap)\n",
    "f.colorbar(points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f62c10-8bd9-4bf7-8b40-c36634e6a15b",
   "metadata": {},
   "source": [
    "## Tune and Optimize kNN in Python Using scikit-learn\n",
    "\n",
    "Can I improve the predictive score from the kNN Performances ?\n",
    "\n",
    "From earlier calculations , we determined that the best K value was 9.\n",
    "\n",
    "Another way to determine the best value for k is using GridSearchCV : \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e9af4-b8db-47f6-b212-8ced0327de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    ">>> parameters = {\"n_neighbors\": range(1, 20)}\n",
    ">>> gridsearch = GridSearchCV(KNeighborsRegressor(), parameters)\n",
    ">>> gridsearch.fit(X_train, y_train)\n",
    "GridSearchCV(estimator=KNeighborsRegressor(),\n",
    "             param_grid={'n_neighbors': range(1, 20),\n",
    "                         'weights': ['uniform', 'distance']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954f1a4-e024-4f68-90fe-82f50e8cc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c581d-dbd3-4da7-8bb7-af3fb0ebe212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we know what the best nearest neighbour is we can see how it affects the train and test analysis\n",
    "\n",
    "train_preds_grid = gridsearch.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, train_preds_grid)\n",
    "train_rmse = sqrt(train_mse)\n",
    "train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2172f-463b-4db0-bf9e-50e849dc3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_grid = gridsearch.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds_grid)\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fef85e-1c94-4310-b6e6-f82f8dc76160",
   "metadata": {},
   "source": [
    "### Adding Weighted Average of Neighbors Based on Distance\n",
    "Below, you’ll test whether the performance of your model will be any better when predicting using a weighted average instead of a regular average. This means that neighbors that are further away will less strongly influence the prediction.\n",
    "\n",
    "You can do this by setting the weights hyperparameter to the value of \"distance\". However, setting this weighted average could have an impact on the optimal value of k. Therefore, you’ll again use GridSearchCV to tell you which type of averaging you should use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9e72a-62f3-408b-8d57-d632bfb766fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \"n_neighbors\": range(1, 20), \"weights\": [\"uniform\", \"distance\"]}\n",
    "gridsearch = GridSearchCV(KNeighborsRegressor(), parameters)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "gridsearch.best_params_\n",
    "test_preds_grid = gridsearch.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds_grid)\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7349fc3-9d54-42a7-a78d-9b57b5159572",
   "metadata": {},
   "source": [
    "### Further Improving on kNN in scikit-learn With Bagging\n",
    "\n",
    "As a third step for kNN tuning, you can use bagging. Bagging is an ensemble method, or a method that takes a relatively straightforward machine learning model and fits a large number of those models with slight variations in each fit. Bagging often uses decision trees, but kNN works perfectly as well.\n",
    "\n",
    "Ensemble methods are often more performant than single models. One model can be wrong from time to time, but the average of a hundred models should be wrong less often. The errors of different individual models are likely to average each other out, and the resulting prediction will be less variable.\n",
    "\n",
    "You can use scikit-learn to apply bagging to your kNN regression using the following steps. First, create the KNeighborsRegressor with the best choices for k and weights that you got from GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ef1e2-5e7e-4606-9207-8a256a664a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = gridsearch.best_params_[\"n_neighbors\"]\n",
    "best_weights = gridsearch.best_params_[\"weights\"]\n",
    "bagged_knn = KNeighborsRegressor( n_neighbors=best_k, weights=best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79003948-c894-4a4c-a2c5-e8efe8382327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagging_model = BaggingRegressor(bagged_knn, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983211e-6b05-4126-83f0-575e4a48ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagging_model = BaggingRegressor(bagged_knn, n_estimators=100).fit(X_train, y_train)\n",
    "test_preds_grid = bagging_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_preds_grid)\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f7862-6cab-4474-be54-57414662ca0f",
   "metadata": {},
   "source": [
    "Conclusions : \n",
    "    \n",
    "Predictive performance of the algorithm : \n",
    "    \n",
    "    Model                             RMSE :Prediction Error\n",
    "    Arbitrary                         0.5626661247145013  \n",
    "    GridsearchCV for k                0.5735040119011083     \n",
    "    GridserchCV for k and weights     0.5690768514853327\n",
    "    Bagging and GridsearchCV          0.5706925692674394\n",
    "    \n",
    "\n",
    "There isnt alot of differences in the errors achieved by tuning and optimizing kNN ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691c973-5a22-4765-bf05-d2e59094924d",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2c2a7-5700-42be-9a2f-a0c441f2b27d",
   "metadata": {},
   "source": [
    "## SciKit-Learn Algorithm - Linear Regression <sup>[23][24]</sup>\n",
    "\n",
    "An example of an supervised Algorithm : use for Classification and Regression\n",
    "\n",
    "Classification - learn how to classify any new observation - target variable is categorical - using accuracy , precision and recall to measure precision of the model algorithm .\n",
    "\n",
    "Regression - prediction made on numerous independent variables vs target variable is ladder score using Coefficients of determination , RMSE and MSE to measure the precision of the model algorithm. It identifies the equation that produces the smallest difference between all the observed values and their fitted values otherwise called residuals\n",
    "\n",
    "The five basic steps : \n",
    "\n",
    "1. Import the packages and classes you need.\n",
    "2. Provide data to work with and eventually do appropriate transformations.\n",
    "3. Create a regression model and fit it with existing data.\n",
    "4. Check the results of model fitting to know whether the model is satisfactory.\n",
    "5. Apply the model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e5882-82a9-4486-a437-58a099468917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a variable lm as the instance of LinearRegression and then fitting a linear model to it \n",
    "# using the data from the earlier data processing into train and test sets\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822bdac-2090-48b7-9d11-815864d7a573",
   "metadata": {},
   "source": [
    "#### Determining the R<sup>2</sup> for each of the variables in order to determine which variable carries the most weight<sup>[25]</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac471633-9770-4d55-a7a1-033b3824fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficients:\\n Social support, Healthy life, Freedom, Generosity, Perceptions of corruption \\n',lm.coef_)\n",
    "print('Intercept:',lm.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b4387-cce8-40d6-bf6c-7017cd6c4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "The variable that carries the least weight is perceptions of corruption , with all the others the same nearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099a30c-31ed-4a8c-a097-822818012625",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lm.predict(X_train)\n",
    "y_test_pred = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782117e3-a422-4690-bd2e-e6cf78a3538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the prediction data & real data for the 10 data entries\n",
    "print('Real Data')\n",
    "print(y_test[:10])\n",
    "print('\\n Predicted Data')\n",
    "print(y_test_pred[:10])\n",
    "print('\\n Diff')\n",
    "print(y_test[:10]-y_test_pred[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a50219-9771-44a8-b13c-4a6bedb18dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation of linearity\n",
    "plt.scatter(y_test,y_test_pred)\n",
    "plt.xlabel('Real data')\n",
    "plt.ylabel('predicted data')\n",
    "plt.title('Relationship between predictor and real data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3fa60-0f5b-40c1-8a4d-80c37c6a09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution from residual using visualisation via a distplot\n",
    "sns.distplot(y_test - y_test_pred)\n",
    "plt.title('Residuals', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af5f7d-d229-4ab5-9107-e760431827e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution from residual\n",
    "residual = (y_test - y_test_pred)\n",
    "sw = stats.shapiro(residual)\n",
    "ks = stats.kstest(residual, 'norm')\n",
    "\n",
    "print('Shapiro-Wilk test ---- statistic: {}, p-value: {}'.format(sw[0],sw[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8849cf9-e8ce-43de-bd1c-6d846d7146f8",
   "metadata": {},
   "source": [
    "Both P -values for each of the tests are greater than 0.05 , therefore we can prove there is a degree normal distribution of residual points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a9f2c-39c6-406d-adc1-6ec9b9d3dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate regression model - R squared\n",
    "\n",
    "# This value shows how good the regression function is at fitting the data : the closet the value is to 1 , the better\n",
    "print('R^2 score:',lm.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8748e2-8d9f-43b5-97f3-cf4c771b6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate regression model - RMSE\n",
    "\n",
    "### RMSE is the standard deviation of residual or prediction errors - measure of how far from the regression line the data points are. \n",
    "\n",
    "## The lower the RMSE the better the model is at making predictions. \n",
    "rmse_training = mean_squared_error(y_true=y_train,y_pred=y_train_pred,squared=False)\n",
    "rmse_test = mean_squared_error(y_true=y_test,y_pred=y_test_pred,squared=False)\n",
    "\n",
    "print('RMSE Training Data: {}'.format(rmse_training))\n",
    "print('RMSE Test Data: {}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70b3ce-d458-4c60-87ac-ef2a184e9670",
   "metadata": {},
   "source": [
    "Conclusions : \n",
    "\n",
    "R^2 score: 0.772241705136514\n",
    "\n",
    "This value shows that the models keeps around 0.7 value. Looking at the prediction data vs the data graph , its shows that the distribution of the data are fitted appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51f472-5fa7-44c4-a369-eea75ad19bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance between model\n",
    "\n",
    "list_model = [['Ridge',linear_model.Ridge()],['Lasso',linear_model.Lasso()],['BayessianRidge',linear_model.BayesianRidge()]]\n",
    "performance_result = {}\n",
    "\n",
    "for model_name,regression_model in list_model:\n",
    "  regression_model.fit(X_train, y_train)\n",
    "  y_train_pred = regression_model.predict(X_train)\n",
    "  y_test_pred = regression_model.predict(X_test)\n",
    "\n",
    "  rmse_training = mean_squared_error(y_true=y_train,y_pred=y_train_pred,squared=False)\n",
    "  rmse_test = mean_squared_error(y_true=y_test,y_pred=y_test_pred,squared=False)\n",
    "\n",
    "  r_score = regression_model.score(X_train, y_train)\n",
    "\n",
    "  performance_result[model_name]={'training':rmse_training,'test':rmse_test,'R^2 score':r_score}\n",
    "\n",
    "performance_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9ea7a-283c-495a-86b4-eccca6c4246f",
   "metadata": {},
   "source": [
    "##### Various model Performance results <sup>[26][27][28]</sup>\n",
    "\n",
    "Looking at 3 types of regression analysis  : \n",
    "    \n",
    " - Ridge - a method of estimating the coefficients of multiple-regression models in scenarios where independent variables are highly correlated.\n",
    " - Lasso - Least Absolute shrinkage and selection order which performs both variable selection and regularization in order to enhance the prediction accuracy of the model by reducing the coefficients to zero if possible\n",
    " - Bayesian Ridge - an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference\n",
    "\n",
    "| RMSE            \t| Training \t| Test \t| R^2  \t|\n",
    "|-----------------\t|----------\t|------\t|------\t|\n",
    "| Ridge           \t| 0.51    \t| 0.61 \t| 0.77 \t|\n",
    "| Lasso           \t| 1.05     \t| 1.13 \t| 0.58 \t|\n",
    "| Bayesian Ridge \t| 0.50    \t| 0.62 \t| 0.77 \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988237a-0ae0-4e4b-8aa3-e5866071a039",
   "metadata": {},
   "source": [
    "### Conclusions : \n",
    "\n",
    " I took the dataset The World Happiness scores and used the following machine learning models to see if I could predict the happiness score.\n",
    " \n",
    "I examined the dataset and then visualised the data , trying to understand the various relationships between the variables examined and the happiness score for each of the countries. \n",
    "\n",
    "I reprocessed the data into training and testing through a 80: 20 ratio. to use \n",
    "\n",
    "From there I chose the following algorithm models : \n",
    "\n",
    " - K- Means Clustering - Training this algorithm to classify the happiness score through the determined cluster groups based on features such as economic production , social suppport , life expectancy , freedom , absense of corruption and generosity. This model was accurate in its classification\n",
    " - KNN - Using RMSE to evaluate the model prediction - the test value was higher then the train value , no difference was seen after using the tuning methods\n",
    " - Linear Regression- training this algorithm to see if there is linear relationship between scores and other variables. there was. Our calculated error for the training and test data was also low meaning the model is good at making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786b822-ef31-45ef-897b-ccd16886dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## **References : \n",
    "\n",
    "1. https://scikit-learn.org/stable/index.html\n",
    "2. https://www.tutorialspoint.com/scikit_learn/scikit_learn_modelling_process.htm\n",
    "3. https://www.analyticsvidhya.com/blog/2020/04/supervised-learning-unsupervised-learning,by Alakh Sethi — April 6, 2020\n",
    "4. https://worldhappiness.report/ed/2021/\n",
    "5. https://happiness-report.s3.amazonaws.com/2021/DataForFigure2.1WHR2021C2.xls - link for dataset used\n",
    "6. https://happiness-report.s3.amazonaws.com/2021/Appendix1WHR2021C2.pdf - Details on the variables examined in analysis\n",
    "7. https://realpython.com/pandas-python-explore-dataset/, by Reka Horvath  Jan 06, 2020\n",
    "8. https://plotly.com/python/choropleth-maps/\n",
    "9. https://chart-studio.plotly.com/~dermi222/2/figure1-world-happiness-index-map-2016/#code\n",
    "10.https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
    "11. https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "12. https://seaborn.pydata.org/generated/seaborn.displot.html\n",
    "13. https://plotly.com/python/line-and-scatter/\n",
    "14. https://en.wikipedia.org/wiki/K-means_clustering\n",
    "15. https://www.coursera.org/projects/clustering-world-happiness-report\n",
    "16. https://medium.com/@evgen.ryzhkov/5-stages-of-data-preprocessing-for-k-means-clustering-b755426f9932,by Evgeniy Ryzhkov — July 6, 2020\n",
    "17. https://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7, by Pulkit Sharma — August 25, 2019\n",
    "18. https://en.wikipedia.org/wiki/Elbow_method_(clustering)\n",
    "19. https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/, Alind Gupta , 09 Feb, 2021\n",
    "20. https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/, Aniruddha Bhandari — April 3, 2020\n",
    "21. https://realpython.com/knn-python/, by Joos Korstanje  Apr 07, 2021\n",
    "22. https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/,Aishwarya Singh — August 22, 2018\n",
    "23. https://www.kaggle.com/hafidzjnp/model-machine-learning-to-predict-happiness, HAFIDZ NDP · august 2021 \n",
    "24. https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn, by Mirko Stojiljković  Apr 15, 2019\n",
    "25. https://statisticsbyjim.com/regression/interpret-r-squared-regression/, Jim Frost \n",
    "26. https://en.wikipedia.org/wiki/Ridge_regression\n",
    "27. https://en.wikipedia.org/wiki/Lasso_(statistics)#:~:text=In%20statistics%20and%20machine%20learning,of%20the%20resulting%20statistical%20model.\n",
    "28. https://en.wikipedia.org/wiki/Bayesian_inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
